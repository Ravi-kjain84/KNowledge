{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN04w5xPDZjidJcANwdVcEm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravi-kjain84/Knowledge/blob/main/22_Using_Machine_Learning_for_Detecting_Liquidity_Risk_in_Banks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Classifier"
      ],
      "metadata": {
        "id": "kWrRz5-qbh4k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvoyKrwyZFo_",
        "outputId": "6a07fa83-30cb-4176-9272-a5b618c395ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8866666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Generating a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=42)\n",
        "\n",
        "# Convert the generated data into a pandas DataFrame\n",
        "df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
        "df['target'] = y\n",
        "\n",
        "# Splitting the DataFrame into features and target variable\n",
        "X_df = df.drop('target', axis=1)\n",
        "y_df = df['target']\n",
        "\n",
        "# Splitting dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Creating the Random Forest model with 100 trees\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fitting the model on training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the labels of the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculating the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single Perceptron\n",
        "\n",
        "A single perceptron is a simple linear classifier used for binary classification tasks. It attempts to find a linear boundary between two classes. Below is an example of how to implement a single perceptron in Python from scratch, including generating sample data for demonstration purposes.\n",
        "\n",
        "We'll use a synthetic dataset with two features to keep it simple and visualize the decision boundary easily if needed.\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "1. **Data Generation**: A synthetic dataset with 100 samples and 2 features is created. A simple rule (`X[:, 0] + X[:, 1] > 0`) determines the class labels to ensure it's linearly separable.\n",
        "\n",
        "2. **Perceptron Class**:\n",
        "   - **`__init__`**: Initializes the perceptron with a learning rate, number of iterations, activation function, weights, and bias.\n",
        "   - **`fit` Method**: Trains the perceptron using the given dataset. It updates the weights and bias based on the perceptron learning rule.\n",
        "   - **`predict` Method**: Uses the trained weights and bias to make predictions on new data.\n",
        "   - **`_unit_step_func`**: The activation function, returning 1 if the input is positive and 0 otherwise.\n",
        "\n",
        "3. **Training and Prediction**: The perceptron is trained on the synthetic dataset, and then predictions are made on the same dataset to demonstrate its accuracy.\n",
        "\n",
        "This example demonstrates the basic functionality of a single perceptron for binary classification. Keep in mind that perceptrons are limited to linearly separable data and might not perform well on complex datasets."
      ],
      "metadata": {
        "id": "CT90Jhtrbura"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generating synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.randn(100, 2)  # 100 samples, 2 features\n",
        "y = np.where(X[:, 0] + X[:, 1] > 0, 1, 0)  # Simple linearly separable rule\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.1, n_iters=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.n_iters = n_iters\n",
        "        self.activation_func = self._unit_step_func\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        y_ = np.array([1 if i > 0 else 0 for i in y])\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
        "                y_predicted = self.activation_func(linear_output)\n",
        "\n",
        "                # Perceptron update rule\n",
        "                update = self.lr * (y_[idx] - y_predicted)\n",
        "                self.weights += update * x_i\n",
        "                self.bias += update\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self.activation_func(linear_output)\n",
        "        return y_predicted\n",
        "\n",
        "    def _unit_step_func(self, x):\n",
        "        return np.where(x > 0, 1, 0)\n",
        "\n",
        "# Training the perceptron\n",
        "perceptron = Perceptron(learning_rate=0.1, n_iters=1000)\n",
        "perceptron.fit(X, y)\n",
        "\n",
        "# Making predictions (for example, using the same dataset for simplicity)\n",
        "predictions = perceptron.predict(X)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = np.mean(predictions == y)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "HdWzD_1taQM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19a3e1a9-df50-4cba-cfc3-d87612620cb9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Perceptrons (Simple Neural Network)\n",
        "\n",
        "For multiple perceptrons, we will use TensorFlow and Keras, which allow for an easy setup of layers in a neural network. We will use the same dataset structure.\n",
        "\n",
        "This example sets up a simple neural network with one hidden layer containing 10 perceptrons (or neurons) and an output layer for binary classification. The model is compiled with the Adam optimizer and binary crossentropy loss function, fitting for binary classification tasks. The `fit` method trains the model for a specified number of epochs."
      ],
      "metadata": {
        "id": "ddtQE7Cedk3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generating some sample data\n",
        "# X: feature matrix, y: target labels\n",
        "np.random.seed(42)\n",
        "X = np.random.randn(100, 2)  # 100 samples, 2 features\n",
        "y = (X[:, 0] + X[:, 1] > 0).astype(int)  # simple linearly separable classification\n",
        "\n",
        "# Splitting the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Building the model\n",
        "model = Sequential([\n",
        "    Dense(units=10, activation='relu', input_shape=(X.shape[1],)),  # Hidden layer with 10 perceptrons\n",
        "    Dense(units=1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=10, validation_split=0.2, verbose=0)\n",
        "\n",
        "# Evaluating the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test Accuracy: {accuracy:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH86VArRdW9-",
        "outputId": "d4039d01-f292-41e1-9137-c0c6bda7eafd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 1.00\n"
          ]
        }
      ]
    }
  ]
}