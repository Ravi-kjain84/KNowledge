import pandas as pd
import numpy as np

# Sample data
data = {
    'column1': ['text1', '1,500', '', '3,000', 'text2', ''],
    'column2': ['2', 'text3', '2,500.50', '', '5', ''],
    'description': ['', 'desc1', '', 'desc2', '', 'desc3']
}

# Creating DataFrame
df = pd.DataFrame(data)

def preprocess_value(value):
    # Check if the value is a string and contains a comma
    if isinstance(value, str) and ',' in value:
        # Remove commas and convert to float
        value = value.replace(',', '')
        try:
            value = float(value)
        except ValueError:
            pass  # Keep the original value if it cannot be converted to float
    return value

# Apply preprocessing to the columns
df['column1'] = df['column1'].apply(preprocess_value)
df['column2'] = df['column2'].apply(preprocess_value)

def concat_or_add(row):
    val1, val2 = row['column1'], row['column2']
    
    if (pd.isna(val1) or val1 == '') and (pd.isna(val2) or val2 == ''):
        return 'N.A.'
    
    if pd.isna(val1) or val1 == '':
        val1 = 0 if isinstance(val2, (int, float)) else ''
    if pd.isna(val2) or val2 == '':
        val2 = 0 if isinstance(val1, (int, float)) else ''
        
    if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):
        return val1 + val2
    else:
        return str(val1) + str(val2)

df['column3'] = df.apply(concat_or_add, axis=1)

# Filter out rows where column3 contains only integer or float values and where the description is not blank
filtered_df = df[df['column3'].apply(lambda x: isinstance(x, (int, float))) & (df['description'].apply(pd.notna) & (df['description'] != ''))]

# Reset index for the filtered DataFrame
filtered_df.reset_index(drop=True, inplace=True)

print(filtered_df)




--------

import pandas as pd
import numpy as np

# Sample data
data = {
    'column1': ['text1', 1.5, '', 3, 'text2', ''],
    'column2': [2, 'text3', 2.5, '', 5, ''],
    'description': ['', 'desc1', '', 'desc2', '', 'desc3']
}

# Creating DataFrame
df = pd.DataFrame(data)

def concat_or_add(row):
    val1, val2 = row['column1'], row['column2']
    
    if (pd.isna(val1) or val1 == '') and (pd.isna(val2) or val2 == ''):
        return 'N.A.'
    
    if pd.isna(val1) or val1 == '':
        val1 = 0 if isinstance(val2, (int, float)) else ''
    if pd.isna(val2) or val2 == '':
        val2 = 0 if isinstance(val1, (int, float)) else ''
        
    if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):
        return val1 + val2
    else:
        return str(val1) + str(val2)

df['column3'] = df.apply(concat_or_add, axis=1)

# Filter out rows where both columns contain float values and where the description is not blank
filtered_df = df[(df['column1'].apply(lambda x: isinstance(x, (int, float))) | df['column2'].apply(lambda x: isinstance(x, (int, float)))) & (df['description'] != '')]

# Reset index for the filtered DataFrame
filtered_df.reset_index(drop=True, inplace=True)

print(filtered_df)



----/

import pandas as pd
import numpy as np

# Sample data
data = {
    'column1': ['text1', 1.5, '', 3, 'text2'],
    'column2': [2, 'text3', 2.5, '', 5]
}

# Creating DataFrame
df = pd.DataFrame(data)

def concat_or_add(row):
    val1, val2 = row['column1'], row['column2']
    
    if pd.isna(val1) or val1 == '':
        val1 = 0 if isinstance(val2, (int, float)) else ''
    if pd.isna(val2) or val2 == '':
        val2 = 0 if isinstance(val1, (int, float)) else ''
        
    if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):
        return val1 + val2
    else:
        return str(val1) + str(val2)

df['column3'] = df.apply(concat_or_add, axis=1)

print(df)





----------

import pandas as pd

def process_invoice_excel(input_excel_path, output_excel_path, column_names):
    # Read the Excel file starting from row 12
    df = pd.read_excel(input_excel_path, skiprows=11, header=None)
    
    # Set the column names explicitly
    df.columns = column_names

    # Remove completely blank rows
    df.dropna(how='all', inplace=True)

    # Remove rows with no relevant data
    df.dropna(subset=[column_names[0]], inplace=True)  # Assuming the first column is mandatory

    # Find the index of the "subtotal" row
    subtotal_index = df[df.apply(lambda row: row.astype(str).str.contains('subtotal', case=False).any(), axis=1)].index

    # If "subtotal" is found, keep rows only before "subtotal"
    if not subtotal_index.empty:
        df = df.loc[:subtotal_index[0] - 1]

    # Reset index for the cleaned DataFrame
    df.reset_index(drop=True, inplace=True)

    # Save the cleaned DataFrame to a new Excel file
    df.to_excel(output_excel_path, index=False)

# Example usage
input_excel_path = 'input_invoice.xlsx'
output_excel_path = 'cleaned_invoice_items.xlsx'
column_names = ["Description", "Quantity", "Price", "Total"]  # Adjust these as needed
process_invoice_excel(input_excel_path, output_excel_path, column_names)





----------

import pandas as pd

def process_invoice_excel(input_excel_path, output_excel_path):
    # Read the Excel file starting from row 12
    df = pd.read_excel(input_excel_path, skiprows=11)

    # Remove completely blank rows
    df.dropna(how='all', inplace=True)

    # Remove rows with no relevant data
    # Assuming 'Description' column is one of the mandatory fields for a valid row
    df.dropna(subset=['Description'], inplace=True)

    # Find the index of the "subtotal" row
    subtotal_index = df[df.apply(lambda row: row.astype(str).str.contains('subtotal', case=False).any(), axis=1)].index

    # If "subtotal" is found, keep rows only before "subtotal"
    if not subtotal_index.empty:
        df = df.loc[:subtotal_index[0] - 1]

    # Reset index for the cleaned DataFrame
    df.reset_index(drop=True, inplace=True)

    # Save the cleaned DataFrame to a new Excel file
    df.to_excel(output_excel_path, index=False)

# Example usage
input_excel_path = 'input_invoice.xlsx'
output_excel_path = 'cleaned_invoice_items.xlsx'
process_invoice_excel(input_excel_path, output_excel_path)










------------------

import pandas as pd

# Sample original data frame
data = {
    'Account': ['A001', 'A002', 'A003', 'A004', 'A005'],
    'Affiliate': ['Affiliate_1', 'Affiliate_None', 'Affiliate_2', 'Affiliate_None', 'Affiliate_3'],
    'Amount': [100, 200, 300, 400, 500]
}
df = pd.DataFrame(data)

# Reference table with account validity
reference_data = {
    'Account': ['A001', 'A002', 'A003', 'A004', 'A005'],
    'IsValid': ['y', 'y', 'n', 'y', 'n']
}
reference_df = pd.DataFrame(reference_data)

# Merge the original data frame with the reference table
merged_df = pd.merge(df, reference_df, on='Account')

# Filter the accounts where IsValid is 'y' and Affiliate is 'Affiliate_None'
invalid_affiliates = merged_df[(merged_df['IsValid'] == 'y') & (merged_df['Affiliate'] == 'Affiliate_None')]

# Get the list of such accounts
invalid_accounts = invalid_affiliates['Account'].tolist()

# Display the list of invalid accounts
print("List of accounts with invalid affiliates:", invalid_accounts)



---------

import os
import pandas as pd

# Example mapping dataframe
mapping_df = pd.DataFrame({
    'EntityId': [1, 2, 3, 4, 5, 'common'],
    'EntityCode': ['A', 'B', 'C', 'D', 'E', 'X']
})

# Path to the main folder containing subfolders with country dataframes
main_folder_path = 'path_to_main_folder'

# Loop through each subfolder
for country_folder in os.listdir(main_folder_path):
    country_folder_path = os.path.join(main_folder_path, country_folder)
    
    if os.path.isdir(country_folder_path):
        # Assuming each subfolder contains a CSV file with the country dataframe
        for file_name in os.listdir(country_folder_path):
            if file_name.endswith('.csv'):
                file_path = os.path.join(country_folder_path, file_name)
                
                # Load the country dataframe
                country_df = pd.read_csv(file_path)
                
                # Filter the mapping dataframe based on EntityId in country dataframe
                entity_ids_in_country = country_df['EntityId'].unique()
                filtered_mapping_df = mapping_df[mapping_df['EntityId'].isin(entity_ids_in_country) | (mapping_df['EntityId'] == 'common')]
                
                # Process the filtered mapping dataframe as needed
                print(f"Filtered mapping dataframe for {country_folder}:")
                print(filtered_mapping_df)

                # If you need to save or further process the filtered_mapping_df, you can do so here

-----------


import pandas as pd
import numpy as np

# Example dataframe
data = {'column': [np.nan, 'abc', np.nan, np.nan, 'abc']}
df = pd.DataFrame(data)

# Identify the single non-null value in the column
unique_values = df['column'].dropna().unique()

# Create the control DataFrame without NULL values
control_df = df.dropna().copy()

# Determine the message for control DataFrame
if len(unique_values) == 1:
    control_df['status'] = 'Unique value found'
    single_value = unique_values[0]
    # Fill NaN values with the single value in the original DataFrame
    df['column'].fillna(single_value, inplace=True)
else:
    control_df['status'] = 'More than one unique non-null value or none at all'

print("Original DataFrame after filling NaNs:")
print(df)
print("\nControl DataFrame:")
print(control_df)

---/--




import pandas as pd
from pandas import ExcelWriter

# Sample DataFrames
main_df = pd.DataFrame({'EntityId1_main': [1, 2, 3, 4, 5], 'EntityId2_main': [10, 20, 30, 40, 50], 'Value': [10, 20, 30, 40, 50]})
df1 = pd.DataFrame({'EntityId1': [1, 2, 3], 'MappedValue1': [100, 200, 300]})
df2 = pd.DataFrame({'Key1': [2, 3, 4], 'Key2': [20, 30, 40], 'MappedValue2': [150, 250, 350]})
df3 = pd.DataFrame({'Id': [1, 3, 5], 'MappedValue3': [110, 210, 310]})
df4 = pd.DataFrame({'Code': [1, 2, 4], 'MappedValue4': [120, 220, 320]})
df5 = pd.DataFrame({'ID': [2, 3, 5], 'MappedValue5': [130, 230, 330]})

mapping_dfs = [
    ('df1', df1, {'main': ['EntityId1_main'], 'map': ['EntityId1']}),
    ('df2', df2, {'main': ['EntityId1_main', 'EntityId2_main'], 'map': ['Key1', 'Key2']}),
    ('df3', df3, {'main': ['EntityId1_main'], 'map': ['Id']}),
    ('df4', df4, {'main': ['EntityId1_main'], 'map': ['Code']}),
    ('df5', df5, {'main': ['EntityId1_main'], 'map': ['ID']})
]

# Create a list to store the unmatched values
unmatched_values_list = []

# Merge main_df with each mapping DataFrame and track unmatched values
for name, mapping_df, keys in mapping_dfs:
    main_keys = keys['main']
    map_keys = keys['map']
    merged_df = main_df.merge(mapping_df, how='left', left_on=main_keys, right_on=map_keys, indicator=True)
    # Identify the rows in main_df that don't have matching rows in the mapping DataFrame
    unmatched = merged_df[merged_df['_merge'] == 'left_only'][main_keys]
    for value in unmatched.itertuples(index=False, name=None):
        unmatched_values_list.append([name, ', '.join(main_keys), ', '.join(map(str, value))])

# Create a DataFrame from the unmatched values list
unmatched_values_df = pd.DataFrame(unmatched_values_list, columns=['DataFrame', 'Key Column', 'Missing Value'])

# Create a summary count table
summary_counts = unmatched_values_df.groupby(['DataFrame', 'Key Column']).size().reset_index(name='Unmatched Count')

# Create a new Excel writer object
with ExcelWriter('control_sheet.xlsx') as writer:
    # Write the unmatched values to a single worksheet
    unmatched_values_df.to_excel(writer, sheet_name='Unmatched Values', index=False)
    
    # Write the summary count table to another worksheet
    summary_counts.to_excel(writer, sheet_name='Summary', index=False)

print("Control sheet with unmatched values and summary count table has been written to 'control_sheet.xlsx'.")





import pandas as pd
from pandas import ExcelWriter

# Sample DataFrames
main_df = pd.DataFrame({'EntityId1_main': [1, 2, 3, 4, 5], 'EntityId2_main': [10, 20, 30, 40, 50], 'Value': [10, 20, 30, 40, 50]})
df1 = pd.DataFrame({'EntityId1': [1, 2, 3], 'MappedValue1': [100, 200, 300]})
df2 = pd.DataFrame({'Key1': [2, 3, 4], 'Key2': [20, 30, 40], 'MappedValue2': [150, 250, 350]})
df3 = pd.DataFrame({'Id': [1, 3, 5], 'MappedValue3': [110, 210, 310]})
df4 = pd.DataFrame({'Code': [1, 2, 4], 'MappedValue4': [120, 220, 320]})
df5 = pd.DataFrame({'ID': [2, 3, 5], 'MappedValue5': [130, 230, 330]})

mapping_dfs = [
    ('df1', df1, {'main': ['EntityId1_main'], 'map': ['EntityId1']}),
    ('df2', df2, {'main': ['EntityId1_main', 'EntityId2_main'], 'map': ['Key1', 'Key2']}),
    ('df3', df3, {'main': ['EntityId1_main'], 'map': ['Id']}),
    ('df4', df4, {'main': ['EntityId1_main'], 'map': ['Code']}),
    ('df5', df5, {'main': ['EntityId1_main'], 'map': ['ID']})
]

# Create a list to store the unmatched values
unmatched_values_list = []

# Merge main_df with each mapping DataFrame and track unmatched values
for name, mapping_df, keys in mapping_dfs:
    main_keys = keys['main']
    map_keys = keys['map']
    merged_df = main_df.merge(mapping_df, how='left', left_on=main_keys, right_on=map_keys, indicator=True)
    # Identify the rows in main_df that don't have matching rows in the mapping DataFrame
    unmatched = merged_df[merged_df['_merge'] == 'left_only'][main_keys]
    for value in unmatched.itertuples(index=False):
        unmatched_values_list.append([name, ', '.join(main_keys), value])

# Create a DataFrame from the unmatched values list
unmatched_values_df = pd.DataFrame(unmatched_values_list, columns=['DataFrame', 'Key Column', 'Missing Value'])

# Create a summary count table
summary_counts = unmatched_values_df.groupby(['DataFrame', 'Key Column']).size().reset_index(name='Unmatched Count')

# Create a new Excel writer object
with ExcelWriter('control_sheet.xlsx') as writer:
    # Write the unmatched values to a single worksheet
    unmatched_values_df.to_excel(writer, sheet_name='Unmatched Values', index=False)
    
    # Write the summary count table to another worksheet
    summary_counts.to_excel(writer, sheet_name='Summary', index=False)

print("Control sheet with unmatched values and summary count table has been written to 'control_sheet.xlsx'.")

----------




import pandas as pd
from pandas import ExcelWriter

# Sample DataFrames
main_df = pd.DataFrame({'EntityId1': [1, 2, 3, 4, 5], 'EntityId2': [10, 20, 30, 40, 50], 'Value': [10, 20, 30, 40, 50]})
df1 = pd.DataFrame({'EntityId1': [1, 2, 3], 'MappedValue1': [100, 200, 300]})
df2 = pd.DataFrame({'Key1': [2, 3, 4], 'Key2': [20, 30, 40], 'MappedValue2': [150, 250, 350]})
df3 = pd.DataFrame({'Id': [1, 3, 5], 'MappedValue3': [110, 210, 310]})
df4 = pd.DataFrame({'Code': [1, 2, 4], 'Details': [10, 20, 30, 40, 50]})
df5 = pd.DataFrame({'ID': [2, 3, 5], 'Description': [1, 2, 3, 4, 5]})

mapping_dfs = [
    ('df1', df1, ['EntityId1']),
    ('df2', df2, ['Key1', 'Key2']),
    ('df3', df3, ['Id']),
    ('df4', df4, ['Code']),
    ('df5', df5, ['ID'])
]

# Create a list to store the unmatched values
unmatched_values_list = []

# Merge main_df with each mapping DataFrame and track unmatched values
for name, mapping_df, keys in mapping_dfs:
    merged_df = main_df.merge(mapping_df, how='left', left_on=keys, right_on=keys)
    # Identify the rows in main_df that don't have matching rows in the mapping DataFrame
    unmatched = merged_df[merged_df[keys[0]].isna()][keys]
    for value in unmatched.itertuples(index=False):
        unmatched_values_list.append([name, ', '.join(keys), tuple(value)])

# Create a DataFrame from the unmatched values list
unmatched_values_df = pd.DataFrame(unmatched_values_list, columns=['DataFrame', 'Key Column', 'Missing Value'])

# Create a summary count table
summary_counts = unmatched_values_df.groupby(['DataFrame', 'Key Column']).size().reset_index(name='Unmatched Count')

# Create a new Excel writer object
with ExcelWriter('control_sheet.xlsx') as writer:
    # Write the unmatched values to a single worksheet
    unmatched_values_df.to_excel(writer, sheet_name='Unmatched Values', index=False)
    
    # Write the summary count table to another worksheet
    summary_counts.to_excel(writer, sheet_name='Summary', index=False)

print("Control sheet with unmatched values and summary count table has been written to 'control_sheet.xlsx'.")

----//////



import pandas as pd
from pandas import ExcelWriter

# Sample DataFrames
main_df = pd.DataFrame({'EntityId': [1, 2, 3, 4, 5], 'Value': [10, 20, 30, 40, 50]})
df1 = pd.DataFrame({'EntityId': [1, 2, 3], 'MappedValue1': [100, 200, 300]})
df2 = pd.DataFrame({'Key': [2, 3, 4], 'MappedValue2': [150, 250, 350]})
df3 = pd.DataFrame({'Id': [1, 3, 5], 'MappedValue3': [110, 210, 310]})
df4 = pd.DataFrame({'Code': [1, 2, 4], 'MappedValue4': [120, 220, 320]})
df5 = pd.DataFrame({'ID': [2, 3, 5], 'MappedValue5': [130, 230, 330]})

mapping_dfs = [
    ('df1', df1, 'EntityId'),
    ('df2', df2, 'Key'),
    ('df3', df3, 'Id'),
    ('df4', df4, 'Code'),
    ('df5', df5, 'ID')
]

# Create a list to store the unmatched values
unmatched_values_list = []

# Merge main_df with each mapping DataFrame and track unmatched values
for name, mapping_df, key in mapping_dfs:
    merged_df = main_df.merge(mapping_df, how='left', left_on='EntityId', right_on=key)
    unmatched = merged_df[merged_df[key].isna()]['EntityId']
    for value in unmatched:
        unmatched_values_list.append([name, key, value])

# Create a DataFrame from the unmatched values list
unmatched_values_df = pd.DataFrame(unmatched_values_list, columns=['DataFrame', 'Key Column', 'Missing Value'])

# Create a summary count table
summary_counts = unmatched_values_df.groupby(['DataFrame', 'Key Column']).size().reset_index(name='Unmatched Count')

# Create a new Excel writer object
with ExcelWriter('control_sheet.xlsx') as writer:
    # Write the unmatched values to a single worksheet
    unmatched_values_df.to_excel(writer, sheet_name='Unmatched Values', index=False)
    
    # Write the summary count table to another worksheet
    summary_counts.to_excel(writer, sheet_name='Summary', index=False)

print("Control sheet with unmatched values and summary count table has been written to 'control_sheet.xlsx'.")

-------------




import pandas as pd
from pandas import ExcelWriter

# Sample DataFrames
main_df = pd.DataFrame({'EntityId': [1, 2, 3, 4, 5], 'Value': [10, 20, 30, 40, 50]})
df1 = pd.DataFrame({'EntityId': [1, 2, 3], 'MappedValue1': [100, 200, 300]})
df2 = pd.DataFrame({'Key': [2, 3, 4], 'MappedValue2': [150, 250, 350]})
df3 = pd.DataFrame({'Id': [1, 3, 5], 'MappedValue3': [110, 210, 310]})
df4 = pd.DataFrame({'Code': [1, 2, 4], 'MappedValue4': [120, 220, 320]})
df5 = pd.DataFrame({'ID': [2, 3, 5], 'MappedValue5': [130, 230, 330]})

mapping_dfs = [
    ('df1', df1, 'EntityId'),
    ('df2', df2, 'Key'),
    ('df3', df3, 'Id'),
    ('df4', df4, 'Code'),
    ('df5', df5, 'ID')
]

# Create a summary count table
summary_counts = []

# Create a dictionary to store unmatched values
unmatched_values = {name: [] for name, _, _ in mapping_dfs}

# Merge main_df with each mapping DataFrame and track unmatched values
for name, mapping_df, key in mapping_dfs:
    merged_df = main_df.merge(mapping_df, how='left', left_on='EntityId', right_on=key)
    unmatched = merged_df[merged_df[key].isna()]['EntityId']
    unmatched_values[name].extend(unmatched.tolist())
    summary_counts.append([name, len(unmatched)])

# Create a summary DataFrame
summary_df = pd.DataFrame(summary_counts, columns=['DataFrame', 'Unmatched Count'])

# Create a new Excel writer object
with ExcelWriter('control_sheet.xlsx') as writer:
    # Write the summary count table
    summary_df.to_excel(writer, sheet_name='Summary', index=False)
    
    # Write the unmatched values to separate worksheets
    for name, values in unmatched_values.items():
        if values:
            unmatched_df = pd.DataFrame(values, columns=['Unmatched EntityId'])
            unmatched_df.to_excel(writer, sheet_name=f'Unmatched_{name}', index=False)

print("Control sheet with unmatched values and summary count table has been written to 'control_sheet.xlsx'.")




-----------


import pandas as pd
from pandas import ExcelWriter

# Sample DataFrames and their names and key columns
df1 = pd.DataFrame({'EntityId': [1, 2, 3, 4, 1], 'Value': [10, 20, 30, 40, 50]})
df2 = pd.DataFrame({'Key1': [5, 6, 7, 8, 5], 'Key2': [1, 2, 3, 4, 1], 'Data': [100, 200, 300, 400, 500]})
df3 = pd.DataFrame({'Id': [9, 10, 11, 12, 12], 'Info': [1000, 2000, 3000, 4000, 5000]})
df4 = pd.DataFrame({'Code': [13, 14, 15, 16, 13], 'Details': [10, 20, 30, 40, 50]})
df5 = pd.DataFrame({'ID': [17, 18, 19, 20, 20], 'Description': [1, 2, 3, 4, 5]})

dataframes = [
    ('df1', df1, ['EntityId']),
    ('df2', df2, ['Key1', 'Key2']),
    ('df3', df3, ['Id']),
    ('df4', df4, ['Code']),
    ('df5', df5, ['ID'])
]

# Consolidated results DataFrame
consolidated_results = []

# Create a new Excel writer object
with ExcelWriter('duplicates_only.xlsx') as writer:
    for name, df, keys in dataframes:
        # Find duplicate rows
        duplicates = df[df.duplicated(subset=keys, keep=False)]
        
        # Count the number of unique items where the total count is greater than 1
        unique_counts = df.groupby(keys).size()
        duplicate_count = unique_counts[unique_counts > 1].count()
        
        # Append the results to the consolidated list
        consolidated_results.append([name, ', '.join(keys), duplicate_count])
        
        # Write the duplicates to the Excel sheet if any
        if not duplicates.empty:
            duplicates.to_excel(writer, sheet_name=f'duplicates_{name}', index=False)

    # Create a DataFrame from the consolidated results
    consolidated_df = pd.DataFrame(consolidated_results, columns=['DataFrame', 'Key Columns', 'Duplicate Count'])
    
    # Write the consolidated results to the Excel sheet
    consolidated_df.to_excel(writer, sheet_name='Consolidated Results', index=False)

print("Duplicates and consolidated results have been written to 'duplicates_only.xlsx'.")

-----------///////------





import pandas as pd
from pandas import ExcelWriter

# Sample DataFrames and their key columns
df1 = pd.DataFrame({'EntityId': [1, 2, 3, 4, 1], 'Value': [10, 20, 30, 40, 50]})
df2 = pd.DataFrame({'Key': [5, 6, 7, 8, 5], 'Data': [100, 200, 300, 400, 500]})
df3 = pd.DataFrame({'Id': [9, 10, 11, 12, 12], 'Info': [1000, 2000, 3000, 4000, 5000]})
df4 = pd.DataFrame({'Code': [13, 14, 15, 16, 13], 'Details': [10, 20, 30, 40, 50]})
df5 = pd.DataFrame({'ID': [17, 18, 19, 20, 20], 'Description': [1, 2, 3, 4, 5]})

dataframes = [
    (df1, 'EntityId'),
    (df2, 'Key'),
    (df3, 'Id'),
    (df4, 'Code'),
    (df5, 'ID')
]

# Create a new Excel writer object
with ExcelWriter('data_with_duplicates.xlsx') as writer:
    for idx, (df, key) in enumerate(dataframes, start=1):
        # Check for duplicates
        duplicates = df[df.duplicated(key, keep=False)]
        
        # Write the original DataFrame to the Excel sheet
        df.to_excel(writer, sheet_name=f'DataFrame{idx}', index=False)
        
        # Write the duplicates to the Excel sheet if any
        if not duplicates.empty:
            duplicates.to_excel(writer, sheet_name=f'Duplicates{idx}', index=False)

print("DataFrames and duplicates have been written to 'data_with_duplicates.xlsx'.")








import pandas as pd

# Sample DataFrame with string and float columns
data = {
    'col1': [1, 2, 3],
    'col2': [4.5, 5.5, 6.5],  # Column to keep all decimals
    'col3': [7, 8, 9],
    'col4': [10, 11, 12],
    'col5': ['A', 'B', 'C'],  # String column
    'col6': [16, 17, 18],
    'col7': ['X', 'Y', 'Z'],  # String column
    'col8': [22, 23, 24],
    'col9': [25, 26, 27],
    'col10': [28.5, 29.5, 30.5]  # Column to round to zero decimals
}

df = pd.DataFrame(data)

# File path
file_path = 'output.txt'

# Header string
header = "My data is here:"

# Specify the column names
float_col_to_round = 'col10'
float_col_keep_decimals = 'col2'

# Write to file with header
with open(file_path, 'w') as file:
    file.write(f'{header}\n')
    for index, row in df.iterrows():
        formatted_values = []
        for col in df.columns:
            value = row[col]
            if col == float_col_to_round:
                formatted_values.append(f'{value:.0f}')  # Round to zero decimals
            else:
                formatted_values.append(f'{value}')
        line = ';'.join(formatted_values)
        file.write(f'{line}\n')

# Display the content of the text file for verification
with open(file_path, 'r') as file:
    print(file.read())





import pandas as pd

# Example DataFrame
df = pd.DataFrame({
    'column2': ['A', 'B', 'C', 'D', 'A', 'F']
})

# Example mapping DataFrame
mapping_df = pd.DataFrame({
    'column2': ['B', 'C', 'D'],
    'column3': ['MappedValue1', 'MappedValue2', 'MappedValue3']
})

# Define a function to apply the logic
def populate_column1(row, mapping_dict):
    if row['column2'] == 'A':
        return 'b'
    elif row['column2'] in mapping_dict:
        return mapping_dict[row['column2']]
    else:
        return 'e'

# Create a dictionary for faster lookup from the mapping DataFrame
mapping_dict = mapping_df.set_index('column2')['column3'].to_dict()

# Apply the function to populate 'column1'
df['column1'] = df.apply(populate_column1, axis=1, mapping_dict=mapping_dict)

print(df)








= let
    Value = [Value],
    Currency = [Currency],
    RangeStart = Number.RoundDown(Value / 50000) * 50000,
    RangeEnd = RangeStart + 49999,
    RangeText = if Value < 5000 then Currency & " less than " & Currency & " 5000" else Currency & " " & Text.From(RangeStart + 1) & " to " & Currency & " " & Text.From(RangeEnd)
in
    RangeText



import pandas as pd

# Sample data for the DataFrame
data = {
    'Categories': ['A', 'B', 'C', 'D'],
    'Values': [10, 20, 30, 40]
}

df = pd.DataFrame(data)

# Create a Pandas Excel writer using XlsxWriter as the engine
with pd.ExcelWriter('data_with_chart.xlsx', engine='xlsxwriter') as writer:
    # Write your DataFrame to an Excel file on Sheet1
    df.to_excel(writer, sheet_name='Sheet1', index=False)

    # Access the XlsxWriter workbook and worksheet objects from the dataframe
    workbook = writer.book
    worksheet = writer.sheets['Sheet1']
    
    # Create a bar chart object
    chart = workbook.add_chart({'type': 'bar'})

    # Configure the series of the chart from the DataFrame data. 
    # Here we need to adjust the cell range based on the DataFrame's size
    chart.add_series({
        'categories': '=Sheet1!$A$2:$A$5',
        'values':     '=Sheet1!$B$2:$B$5',
    })

    # Optionally, add chart title, axis titles, etc.
    chart.set_title({'name': 'Values by Category'})
    chart.set_x_axis({'name': 'Category'})
    chart.set_y_axis({'name': 'Value'})

    # Insert the chart into the worksheet with an offset
    worksheet.insert_chart('D2', chart)

# Note: The Excel file 'data_with_chart.xlsx' is saved in your current directory.