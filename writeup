Subject: Request for Your Guidance on Testing Strategies in Agile Framework

Dear David,

I hope this email finds you well.

Bhanu suggested I reach out to you for your expertise in Agile methodologies, as I am currently exploring opportunities for a Testing Center of Excellence (TCOE) role and would greatly value your guidance.

I am particularly interested in understanding testing strategies that are well-aligned with the Agile framework. To this end, I am preparing a presentation to outline how testing best practices can be effectively integrated into Agile methodologies. Before finalizing my approach, I would greatly appreciate your feedback to ensure that it aligns with Agile principles and reflects practical insights.

Would it be possible to schedule a quick call on Friday at your convenience? Your advice would be incredibly valuable as I shape my perspective for this opportunity.

Please let me know if Friday works for you or if there’s another time that would be better for your schedule.

Looking forward to hearing from you.

Warm regards,
[Your Full Name]
[Your Contact Information, if necessary]

......

Subject: Seeking Your Guidance on Testing Strategies in Agile Framework

Dear David,

I hope you’re doing well.

Bhanu recommended that I connect with you regarding my current work on testing strategies within the Agile framework. I am exploring opportunities for a Testing Center of Excellence (TCOE) role and am preparing a presentation to examine how testing best practices can integrate seamlessly with Agile methodologies.

Your expertise in Agile would be invaluable in helping me ensure that my approach is practical and aligns with the framework’s principles. Would you be available for a quick call this Friday to discuss this?

Please let me know if Friday works or if another day would be more convenient for you.

Thank you in advance for your time and insights.

Best regards,
[Your Full Name]
[Your Contact Information, if necessary]

......
Here is the updated section incorporating pivoting and unpivoting columns as a complexity parameter:

3.2 Complexity Parameters

The following parameters are used to estimate the complexity of the process:
	1.	Number of Data Enrichment Files:
	•	The number of input files used for lookup, filtering, or enrichment.
	2.	Allocation Rules:
	•	Whether allocation or proportion logic is required.
	3.	Aggregation Requirements:
	•	Steps needed to consolidate data for the final output.
	4.	Control Requirements:
	•	Number and type of validations/reconciliations in pre- and post-control books.
	5.	Mapping Tables:
	•	The number of mapping tables used for lookups and enrichment.
	6.	Pivoting and Unpivoting Columns:
	•	The requirement to pivot or unpivot data columns based on specific categorical columns.
	•	Time estimation: Each column pivoted or unpivoted will take approximately 15 minutes.

5. Estimation Calculation Table

The updated calculation table, including pivoting/unpivoting complexity, is as follows:

Activity	Unit of Work	Time per Unit (minutes)	Comments
Column Copying	Per column	10	Simple copy operation.
Lookup for Enrichment	Per column	20	Includes mapping table lookups.
Filtering	Per condition	15	Based on specific criteria.
Allocation	Per rule	25	Allocation or proportion calculations.
Aggregation for Uploader	Per dataset	40	Includes consolidation steps.
Reconciliation (Post-Control)	Per control point	30	Data comparison and validation.
Pivoting/Unpivoting Columns	Per column	15	Based on specific categorical columns.

This update reflects the inclusion of pivoting/unpivoting columns as a distinct complexity parameter, with a clear time estimate. Let me know if you’d like further refinements!



-----

Estimation Methodology for Solution Development Using Rule and Control Books

Abstract

The purpose of this paper is to develop a structured methodology for estimating the time required to create solutions involving rule books and control books for data adjustment processes. The estimation framework considers inputs from domain experts, the complexity of the process, and detailed operations required for data transformation and validation. This methodology provides a systematic approach to time estimation by breaking down the process into manageable components and assigning time estimates based on defined complexity parameters.

1. Introduction

In data processing and adjustment workflows, creating solutions for automating manual processes involves detailed rule definitions and control frameworks. These solutions typically comprise a rule book that outlines data transformation rules and control books to validate data integrity at both pre- and post-transformation stages. Estimating the time required to design and implement such solutions is a critical step to plan resources and manage stakeholder expectations effectively. This paper outlines a methodology to estimate development time based on the complexity and nature of tasks involved.

2. Objective

The objective of this paper is to define a methodology for estimating the time required to:
	1.	Write the rules in the rule book, including data enrichment, filtering, aggregation, and calculations.
	2.	Develop pre- and post-control books for data validation and reconciliation.
	3.	Quantify the impact of key complexity parameters on the time estimation process.

3. Methodology

3.1 Process Overview

	1.	Expert Consultation:
	•	Domain experts manually performing the data adjustment process will provide detailed explanations of the workflow.
	•	Inputs include:
	•	Input files used (transaction data, mapping files, filters, etc.).
	•	Specific rules for copying, enriching, filtering, and calculating data.
	•	Allocation and aggregation rules to create final output.
	2.	Rule Book Creation:
	•	Documenting data processing rules, such as:
	•	Identifying input columns for copying.
	•	Lookup operations for data enrichment.
	•	Filters applied for specific conditions.
	•	Allocation and proportion rules.
	•	Aggregation and transformation steps.
	3.	Control Book Creation:
	•	Pre-Control Book:
	•	Validations on raw input data (e.g., null value checks, specific mappings, or range validations).
	•	Post-Control Book:
	•	Validations and reconciliations on output data, including:
	•	Comparing aggregated outputs with input data.
	•	Verifying consolidated values.

3.2 Complexity Parameters

The following parameters are used to estimate the complexity of the process:
	1.	Number of Data Enrichment Files:
	•	The number of input files used for lookup, filtering, or enrichment.
	2.	Allocation Rules:
	•	Whether allocation or proportion logic is required.
	3.	Aggregation Requirements:
	•	Steps needed to consolidate data for the final output.
	4.	Control Requirements:
	•	Number and type of validations/reconciliations in pre- and post-control books.
	5.	Mapping Tables:
	•	The number of mapping tables used for lookups and enrichment.

3.3 Time Estimation Framework

Each activity is assigned a standard time estimate based on its complexity. For example:
	•	Column Copying: 10 minutes per column.
	•	Lookup Operations: 20 minutes per column.
	•	Reconciliation Tasks: 30 minutes per control.
	•	Pivoting/Unpivoting Data: 30 minutes per operation.

Time estimates will be summed based on the total number of operations identified during the expert consultation phase.

4. Assumptions

The following assumptions underpin the methodology:
	1.	The domain expert can articulate the process steps clearly without ambiguities.
	2.	Input files and their schemas are available and well-documented.
	3.	Time estimates are based on average processing times observed in prior implementations.
	4.	Any unforeseen complexities (e.g., missing input data) will be addressed through additional consultation, impacting the time estimate.

5. Estimation Calculation Table

The following table outlines time estimates for individual tasks based on complexity parameters:

Activity	Unit of Work	Time per Unit (minutes)	Comments
Column Copying	Per column	10	Simple copy operation.
Lookup for Enrichment	Per column	20	Includes mapping table lookups.
Filtering	Per condition	15	Based on specific criteria.
Allocation	Per rule	25	Allocation or proportion calculations.
Aggregation for Uploader	Per dataset	40	Includes consolidation steps.
Reconciliation (Post-Control)	Per control point	30	Data comparison and validation.
Pivoting/Unpivoting Columns	Per operation	30	Reformatting data structure.

6. Conclusion

This paper provides a structured approach for estimating the time required to create rule books and control books for data adjustment processes. The methodology incorporates expert inputs, defined complexity parameters, and standard time estimates for each activity. This framework can be adapted for similar projects to ensure accurate and reliable time planning.


-----
import os
import shutil
import re
from openpyxl import load_workbook

# Define the original and temporary folder paths
original_folder = 'path/to/original/folder'
temp_folder = 'path/to/temporary/folder'

# Define the list of keywords to search within formulas
search_keywords = ["getValue", "retrieveData", "calculateTotal"]

# Compile regex patterns for each keyword to find matches in formulas
patterns = {keyword: re.compile(rf"\b{re.escape(keyword)}\b", re.IGNORECASE) for keyword in search_keywords}

# Create a temporary folder to store non-password-protected copies
if not os.path.exists(temp_folder):
    os.makedirs(temp_folder)
    print(f"Created temporary folder at '{temp_folder}'")
else:
    print(f"Temporary folder '{temp_folder}' already exists")

# Copy all files to the temporary folder, keeping the folder structure
print("Starting file copy process...")
for root, _, files in os.walk(original_folder):
    for file_name in files:
        # Check if the file is an Excel file
        if file_name.endswith('.xlsx') or file_name.endswith('.xlsm'):
            # Construct full file paths
            src = os.path.join(root, file_name)
            
            # Preserve folder structure in the temporary folder
            relative_path = os.path.relpath(root, original_folder)
            dest_dir = os.path.join(temp_folder, relative_path)
            if not os.path.exists(dest_dir):
                os.makedirs(dest_dir)
                print(f"Created directory '{dest_dir}' in temporary folder")

            dest = os.path.join(dest_dir, file_name)
            
            # Copy the file to the corresponding location in the temporary folder
            shutil.copy(src, dest)
            print(f"Copied '{file_name}' to temporary folder")

print("File copy process completed. Starting search for keywords...")

# Dictionary to store files containing any of the keywords, with the matched keyword(s)
files_with_keywords = {}

# Now iterate through the copied files in the temporary folder
for root, _, files in os.walk(temp_folder):
    for file_name in files:
        file_path = os.path.join(root, file_name)
        print(f"Checking file '{file_name}' for keywords {search_keywords}...")

        # Try opening the workbook, skipping if it's password-protected
        try:
            workbook = load_workbook(file_path, data_only=False)  # Load formulas as they are
            print(f"Opened workbook '{file_name}' successfully")
        except Exception as e:
            print(f"Could not open '{file_name}' due to password protection or other error.")
            continue

        # Search for each keyword in formulas in each sheet
        found_keywords = set()
        for sheet in workbook.sheetnames:
            print(f"Searching in sheet '{sheet}' of '{file_name}'...")
            worksheet = workbook[sheet]
            for row in worksheet.iter_rows():
                for cell in row:
                    if cell.data_type == 'f':  # Only check cells with formulas
                        cell_value = str(cell.value)
                        # Check each pattern to see if any keyword matches
                        for keyword, pattern in patterns.items():
                            if pattern.search(cell_value):
                                found_keywords.add(keyword)
                                print(f"Keyword '{keyword}' found in '{file_name}' in sheet '{sheet}'")
                                break  # Stop checking other keywords for this cell
                if found_keywords:
                    break  # Stop further search in this sheet if a keyword is found
            if found_keywords:
                break  # Stop further search in this file if a keyword is found
        workbook.close()

        # If any keywords were found, update the results dictionary
        if found_keywords:
            files_with_keywords[file_name] = list(found_keywords)
            print(f"File '{file_name}' updated in the results list with matched keywords: {found_keywords}")

print("Search completed.")

# Output the results
if files_with_keywords:
    print("\nThe following files contain formulas with the specified keywords:")
    for file, keywords in files_with_keywords.items():
        print(f"File: {file} - Matched Keywords: {', '.join(keywords)}")
else:
    print("No files containing the specified keywords were found.")